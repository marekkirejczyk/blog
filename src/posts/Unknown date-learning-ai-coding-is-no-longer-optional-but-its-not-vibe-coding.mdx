---
title: "Learning AI coding is no longer optional. But itâ€™s not vibe coding."
author: "Unknown author"
date: "Unknown date"
heroImage: "Unknown image"
# tags: [] # No tags generated
---

This is second blog post in the series about AI augmented coding:

- ğŸ‘» Part 1: Myths, misconceptions, and realistic expectations about AI-assisted coding
- ğŸ§©** Part 2: Tools, fundamentals and step-by-step workflowx**
- ğŸš€ Part 3: Advanced techniques (coming soon)

There are now several **categories of AI tooling for software development**, and understanding the roles they play is crucial to unlocking their full potential.

#### LLMs
First, we have **general-purpose large language models (LLMs)** like [ChatGPT](https://chatgpt.com/) , [Perplexity](https://www.perplexity.ai/) , [Sonnet](https://www.anthropic.com/claude/sonnet) , and several others. They excel at:

- answering quick technical questions,
- generating code snippets,
- assisting with research, and
- preparing documentation.

Most importantly, **LLMs act as the ** [**operating system**]
behind many other AI development tools. Mastering how to work with them is essential to unlock the *â€œmultiplier modeâ€* of productivity.

#### Vibe Coding Prototypes
For **vibe coding prototypes**, we use [**Lovable**](https://lovable.dev/) . There are several competitors in this space, including [**Replit**]
and [**n8n**](https://n8n.io/) .

Lovableâ€Šâ€”â€Šlike others in this categoryâ€Šâ€”â€Šis highly opinionated. It relies on a single stack (**React + Supabase**) and is limited to creating **web applications** with a heavy front-end but no serious backend.

What makes it remarkable is the **speed and completeness of the output**. With a single prompt, Lovable can:

- determine the style and theme of the application,
- outline the exact features,
- generate boilerplate code,
- prepare an implementation plan,
- build the features, and
- fix any bugs along the way.

From there, you can keep prompting it to add features or modify existing ones. You donâ€™t need to know anything about programming to **vibe code** a non-trivial web application.

Another cool feature is that **each promptâ€™s result is committed to a GitHub repository**. This means you can:

- easily undo or roll back changes,
- step in to modify the code manually, and
- even **mix vibe coding with AI-augmented coding** when you want more control or customization.

#### Code Review
For code reviews, we are currently using [**CodeRabbit**](https://www.coderabbit.ai/) , and overall weâ€™re quite happy with it. It integrates directly with GitHub and leaves comments on Pull Requests.

Not every comment is valuable, but often it:

- catches bugs ahead of time,
- proposes cleaner syntax, and
- suggests incremental improvements.

It does **not replace traditional code review**, but itâ€™s a reliable daily companion that provides practical, actionable pointers.

One of its strengths is the **low entry barrier**â€Šâ€”â€Šyou donâ€™t need to learn new workflows or build new habits to start benefiting from it.

âš ï¸ **Note:** youâ€™ll need a paid version of CodeRabbit to get useful comments. The free version tends to output â€œpoems,â€ which can be discouraging. It tends to be overly verbose out of the box, so a little configuration is needed to make it more usable and focused on quality.

#### Agent Systems (Assistants)
We are currently using two agent systems: **Cursor** and **Claude Code**. At a high level, they follow the same idea: an AI agent that assists with coding.

- **Cursor** integrates directly into the development environment. Itâ€™s a Visual Studio Code fork, which I personally find very convenient.
- **Claude Code** is often preferred by people who code more than I doâ€Šâ€”â€Šit tends to be more accurate, gathers more context, and supports longer reasoning.

In practice, we combine both, depending on the task. Some developers experiment with other tools as they emerge, which is also great to see.

Under the hood, these agent systems rely on **LLMs** that are **swappable**. At this time, we recommend using **Sonnet** with both Claude and Cursor, though I see many people are switching to **Opus **lately.

#### Tooling Summary
There are three key things to keep in mind about AI tooling:

1. **No single tool is enough.**It usually takes a combination of tools to unlock the full potential of AI-assisted coding.
2. **Tools are intuitive, but deeper than they look.**Most are easy to start with, yet each comes with a learning curve. Once you dig in, youâ€™ll discover they offer far more than meets the eye.
3. **The landscape changes constantly.**New tools appear all the time, and itâ€™s nearly impossible to stay fully up to date. My advice mirrors what I recommend for coding languages and frameworks:ğŸ‘‰ **Master one stack **so you can later pick up others quickly.ğŸ‘‰ If you want to get started fast, use the stack described in this sectionâ€Šâ€”â€Šitâ€™s a solid entry point.

The first and most important thing to understand about working with LLMs and agent systems is the **role they should play in your workflow**.

LLMs are strange. On one hand, they contain more knowledge than any single person in the world. On the other hand, they hallucinateâ€Šâ€”â€Šand can be very convincing when they do. So how should you work with such a bizarre persona?

#### The Eager Intern Analogy
A popular analogy is to think of AI tools as eager interns.

- An eager intern can produce a vast amount of code, but it may be buggy, poorly designed, or misaligned with the existing codebase.
- Your role is to teach the intern how to code your way, while preventing serious mistakes.
- If youâ€™ve worked with interns before, the feeling will be familiar.

This analogy also highlights the current limitations of AI-assisted coding. Over time, with proper supervision, AI will begin to â€œconvergeâ€ to your coding style, stack, and architectural decisions. In other words, the quality of the codebase will only ever be as good as the standards you enforce.

> By contrast, pure *vibe coding*â€Šâ€”â€Šwill likely produce buggy, insecure systems that are painful to maintain long-term. This is an example of phenomenon called AI slop.
#### Throughput and Multipliers
The throughput limit with AI is similar to how many interns a good engineer can manage. In practice, thatâ€™s usually **2â€“3 interns** at once. It takes time to discuss features, align on design, and review code before merging. But once expectations are clear, such a setup can be very productiveâ€Šâ€”â€Šoften yielding a **2â€“3Ã— productivity multiplier**, which matches the practical improvements most engineers report with AI.

#### Upsides of AI vs. Real Interns
AI assistants have advantages over their human counterparts:

- They donâ€™t complain, take time off, switch jobs, or ask for a raise.
- You donâ€™t need to re-teach each â€œinternâ€ what the last one already learned.
- Thereâ€™s no need to sync between themâ€Šâ€”â€Šno planning sessions or retros required.

This means one engineer can often achieve as much as a three-person team. A three-person team can reach the output of six to nine engineers, and so on.

> This also explains why itâ€™s becoming **so hard to get a first job in software development.**
Thatâ€™s why **investing in learning AI-assisted coding is worth it**. And since AI continues to improveâ€Šâ€”â€Šoften with significant new releases every monthâ€Šâ€”â€Šthe productivity multiplier will only grow.

Now letâ€™s look at the basic techniques you need to master as a prerequisite for an AI-augmented workflow.

Great engineersâ€Šâ€”â€Šwhether consciously or notâ€Šâ€”â€Šdivide their work into **two phases: planning and implementation**.

- **Planning** touches on many areas: research, product decisions, software architecture and design, technology stack choices, and the selection of key algorithms.
- **Implementation** is the hands-on coding: writing functions, deciding what arguments to pass, designing test cases, and handling all the micro-decisions of development.

Experienced developers often move fluidly between the two. Strategic ideas may surface under the shower, while research happens casually over lunch talks with colleagues.

With **LLMs and agent systems** (our â€œeager internsâ€), however, this boundary needs to become **more explicit**.

#### Planning First
A typical AI-augmented workflow begins with the planning phase, which usually means creating a document: plan, design doc, a Product Requirement Document (PRD) or whatever we call it.

This document should define the **scope of the feature**, specify the **technologies** and **patterns** to use, outline **algorithms**, and describe the **testing approach**.

This technique applies both to building new features and to bootstrapping new projects. That said, setting up an existing project for AI-assisted workflows differs from creating a new one from scratch. Iâ€™ll cover these nuances in the next sectionsâ€Šâ€”â€Šand in the following blog post, respectively.

#### The Good News
Agent systems are steadily improving at **planning**. They increasingly ask the right questions, suggest approaches, and even perform research on their own. This means that the heavy lifting of moving between planning and implementation will keep getting easier over time.

Still, the art of writing solid yet concise planning documents is crucial to AI coding.

This brings us to the next technique: creating docs.

Writing documentation can feel daunting. If only there were an AI assistant to help with that â€¦ oh waitâ€Šâ€”â€Šthere is! LLMs, of course.

Here are the essentials:

1. **Start with a clear prompt: **Describe what you want built: scope/constraints/success criteria, related technical details. Also ask the LLM to analyze related existing code (if available) and to research best practices for the technologies or patterns involved.
2. **Ask for clarifying questions first. ***In the same prompt*, tell the LLM to ask questions before answering (e.g., â€œAsk me clarifying questions first.â€). Many agent systems do this, but being explicit works betterâ€Šâ€”â€Šyou know how eager interns can beâ€¦
3. **Require an implementation plan with TODOs. **If the draft lacks a step-by-step plan, ask for one explicitly.
4. **Iterate on the draft**ğŸ‘‰ In **ChatGPT:** type â€œAdd to canvas,â€ then refine with follow-up prompts (works for code, too).ğŸ‘‰ **Cursor / Claude:** save a .md file (e.g., plan.md) and keep asking the tool to update that file.Donâ€™t expect perfection from the first prompt. Create a draft, then refineâ€Šâ€”â€Šand keep it concise, because youâ€™ll need to review it carefully.
5. **Edit via whichever path is fastest. **Ask the LLM to modify the doc or edit it yourself. Speed > purity.
6. **Always review end-to-end. **Make sure the final doc says exactly what you intend to build. I canâ€™t stress the importance of this enough.

> **Watch-out:*** Even a simple request like â€œCreate a PRD for project Xâ€¦â€ can produce a long document with a mix of gold and noise. Thatâ€™s why the next fundamental technique is ***working in small chunks***.*
LLMs can **hallucinate** and are **eager to generate** lots of documentation and code. Your job is to **gatekeep quality** and drive a tight iterate-review loop.

For a new project, itâ€™s normal to spend a few hours researching the **tech stack**, **design options**, and **best practices**â€Šâ€”â€Šand to keep iterating on the document.

After each substantial addition, **commit** so itâ€™s easy to go back and review the **diff** of recent changes. Itâ€™s normal to make **a few to several commits per hour**; keep them **small and descriptive**.

The diff is the primary lens for collaborating with LLMs. Review each change before accepting it.

#### Implementation
When moving to the implementation phase, working in small chunks becomes even more important.

When starting out with AI-assisted coding, a best practice is to instruct the assistant to handle one TODO item at a time, and only proceed to the next step after youâ€™ve approved the current one.

When reviewing a chunk of work:

1. Check the patch carefullyâ€Šâ€”â€Šalways start with the diff.
2. Run linters/formatters
3. Verify test coverageâ€Šâ€”â€Šensure the code is covered by automated tests and that all tests pass.
4. Commit the chunkâ€Šâ€”â€Šonly after everything checks out.

Iâ€™ll dive deeper into **automating linting and testing** in later.

You wonâ€™t get predictable results from agent systems if the project isnâ€™t properly set up. Most modern tools rely on **special configuration files**, written in a language you might recognizeâ€¦ plain English. **Claude** uses *claude.md, ***Cursor** uses *.cursorrules. *Other tools have their own equivalents

#### What to Include
These files should describe:
ğŸ‘‰ the purpose of the project and its main functionalities,
ğŸ‘‰ its structure, tech stack (lib versions!), and architecture/design decisions and used patterns/idioms
ğŸ‘‰ the expected workflow (e.g., â€œ*run tests and linter after each major change*â€),
ğŸ‘‰ and high-level expectations (e.g., â€œ*ask clarifying questions*,â€ â€œ*work in small batches*,â€ â€œ*prepare and follow TODO lists*â€).

> That might sound boringâ€¦ but if only there were a tool for writing such files. Oh waitâ€Šâ€”â€Šthere is! ğŸ™‚ Just ask your AI to generate the first version, then iterate on it as described in the previous section about docs.
For inspiration, you can also check out [this repository](https://github.com/PatrickJS/awesome-cursorrules) .

These files prevent frustration when the AI:
 ğŸ˜µ generates code that doesnâ€™t fit your design or style,
 ğŸ˜µ breaks tests, or
 ğŸ˜µ flickers between different libraries or conventions.

With a proper setup, the AI can build features, run tests and linting, and fix issues on its ownâ€Šâ€”â€Šwhile reducing hallucinations and inconsistencies.

> Even with guardrails, the AI wonâ€™t follow rules perfectly. Think of it like an intern: enthusiastic but forgetful. Rules will be followed **most of the time**, but expect occasional lapsesâ€Šâ€”â€Šespecially as conversations get long and context runs out.
#### Working with Monorepos
For monorepos, use **multiple configuration files**:

- **Top-level file:** describes the repoâ€™s structure, high-level roles of each project, how they connect, plus build, CI, and release setup.
- **Per-project file:** describes the tech stack and detailed functionality of each project.

As you combine the techniques described above, the **workflow starts to reveal itself**. Once the project is properly set up, you can begin efficient work on new features.

Each feature follows the same rhythm:

1. **Planning phase** â†’ produce a clear TODO list.
2. **Implementation** â†’ let the AI handle one item at a time, as a self-contained chunk of functionality.
3. **Review & checks** â†’ carefully inspect the diff, run tests and linters.
4. **Commit** â†’ only after everything passes.

Iâ€™ll continue with a handful of more advanced techniques in the next post: *Part 3: Advanced Techniques*. Stay tuned.

*Thank you for reading!*

*If you like this postâ€Šâ€”â€Šhit ğŸ‘ button below.*

*To get notification about the next parts click ***follow ***below.*

*If you want to continue the conversation and see more stuff like this, you can find me on T*witter: [@zkmarek](https://x.com/zkmarek) .